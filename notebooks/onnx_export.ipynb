{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e815f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d2e83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import zipfile\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import random\n",
    "import onnxruntime as ort\n",
    "import onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5178ce09",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71e3df00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting: 100%|██████████| 725/725 [00:08<00:00, 82.50it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted to data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "zip_path = \"data/Diddata.zip\"\n",
    "extract_to = \"data\"\n",
    "\n",
    "os.makedirs(extract_to, exist_ok=True)\n",
    "\n",
    "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "    members = zip_ref.namelist()\n",
    "    for member in tqdm(members, desc=\"Extracting\"):\n",
    "        zip_ref.extract(member, extract_to)\n",
    "\n",
    "print(f\"Extracted to {extract_to}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fdd6af70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Renamed 55 directories to 1-55\n"
     ]
    }
   ],
   "source": [
    "base_dir = \"data/Diddata\"\n",
    "dirs = [d for d in os.listdir(base_dir) if os.path.isdir(os.path.join(base_dir, d))]\n",
    "dirs.sort()  # Sort for consistent numbering\n",
    "\n",
    "for idx, dirname in enumerate(dirs, start=1):\n",
    "    src = os.path.join(base_dir, dirname)\n",
    "    dst = os.path.join(base_dir, str(idx))\n",
    "    os.rename(src, dst)\n",
    "\n",
    "print(f\"Renamed {len(dirs)} directories to 1-{len(dirs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8ede28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the SiameseNetwork class (must match your training code)\n",
    "class LightweightCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=3, stride=2, padding=1),  \n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),  \n",
    "\n",
    "            nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),  \n",
    "\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),  \n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1), \n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2), \n",
    "        )\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1)) \n",
    "        self.out_dim = 128 \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)  \n",
    "        x = self.avgpool(x) \n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        return x \n",
    "class SiameseNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.cnn = LightweightCNN()\n",
    "  \n",
    "        self.project = nn.Linear(128, 512)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(512 * 2, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.3),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        f1 = self.cnn(x1) \n",
    "        f1 = self.project(f1)  \n",
    "        f2 = self.cnn(x2)  \n",
    "        f2 = self.project(f2)  \n",
    "        out = torch.cat([f1, f2], dim=1) \n",
    "        out = self.fc(out)\n",
    "        return out.squeeze(1)\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Instantiate the model\n",
    "model = SiameseNetwork().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a5e423a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model exported to Model/best_kyc_siamese.onnx using real images:\n",
      "Doc: data/Diddata/1/ID_1.jpg\n",
      "Selfie: data/Diddata/1/Selfie_8.jpg\n"
     ]
    }
   ],
   "source": [
    "# 1. Define the same transform as in training\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((112,112)),  # Resize to 112x112\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
    "])\n",
    "\n",
    "# 2. Find a real doc and selfie image\n",
    "base_dir = \"data/Diddata\"\n",
    "person_dirs = sorted([os.path.join(base_dir, d) for d in os.listdir(base_dir) if os.path.isdir(os.path.join(base_dir, d))])\n",
    "\n",
    "doc_img_path, selfie_img_path = None, None\n",
    "for person_dir in person_dirs:\n",
    "    files = os.listdir(person_dir)\n",
    "    doc_candidates = [os.path.join(person_dir, f) for f in files if any(x in f.lower() for x in [\"id\", \"passport\", \"national\"])]\n",
    "    selfie_candidates = [os.path.join(person_dir, f) for f in files if \"selfie\" in f.lower()]\n",
    "    if doc_candidates and selfie_candidates:\n",
    "        doc_img_path = doc_candidates[0]\n",
    "        selfie_img_path = selfie_candidates[0]\n",
    "        break\n",
    "\n",
    "assert doc_img_path is not None and selfie_img_path is not None, \"No doc/selfie pair found!\"\n",
    "\n",
    "# 3. Load and preprocess images\n",
    "doc_img = Image.open(doc_img_path).convert(\"RGB\")\n",
    "selfie_img = Image.open(selfie_img_path).convert(\"RGB\")\n",
    "doc_tensor = transform(doc_img).unsqueeze(0).to(device)     \n",
    "selfie_tensor = transform(selfie_img).unsqueeze(0).to(device)\n",
    "\n",
    "# 4. Load model and export to ONNX\n",
    "model.load_state_dict(torch.load(\"Model/best_kyc_siamese.pt\", map_location=device))\n",
    "model.eval()\n",
    "\n",
    "class ExportableSiameseNetwork(nn.Module):\n",
    "    def __init__(self, base_model):\n",
    "        super().__init__()\n",
    "        self.base_model = base_model\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        logits = self.base_model(x1, x2)\n",
    "        return torch.sigmoid(logits)  # Adds sigmoid to get probability\n",
    "\n",
    "# 4. Create exportable model\n",
    "export_model = ExportableSiameseNetwork(model).to(device)\n",
    "\n",
    "onnx_path = \"Model/best_kyc_siamese.onnx\"\n",
    "torch.onnx.export(\n",
    "    export_model,\n",
    "    (doc_tensor, selfie_tensor),\n",
    "    onnx_path,\n",
    "    export_params=True,\n",
    "    do_constant_folding=True,\n",
    "    input_names=[\"doc_img\", \"selfie_img\"],\n",
    "    output_names=[\"output\"],\n",
    "    dynamic_axes={\n",
    "        \"doc_img\": {0: \"batch_size\"},\n",
    "        \"selfie_img\": {0: \"batch_size\"},\n",
    "        \"output\": {0: \"batch_size\"}\n",
    "    },\n",
    "    opset_version=12\n",
    ")\n",
    "\n",
    "print(f\"Model exported to {onnx_path} using real images:\\nDoc: {doc_img_path}\\nSelfie: {selfie_img_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "80053856",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Define the same transform as in training\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((112,112)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1f255523",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "base_dir = \"data/Diddata\"\n",
    "person_dirs = [os.path.join(base_dir, d) for d in os.listdir(base_dir) if os.path.isdir(os.path.join(base_dir, d))]\n",
    "random.shuffle(person_dirs)\n",
    "\n",
    "pair_type = random.choice([\"positive\", \"negative\"])\n",
    "\n",
    "if pair_type == \"positive\":\n",
    "    # Positive: doc and selfie from the same person\n",
    "    for person_dir in person_dirs:\n",
    "        files = os.listdir(person_dir)\n",
    "        doc_candidates = [os.path.join(person_dir, f) for f in files if any(x in f.lower() for x in [\"id\", \"passport\", \"national\"])]\n",
    "        selfie_candidates = [os.path.join(person_dir, f) for f in files if \"selfie\" in f.lower()]\n",
    "        if doc_candidates and selfie_candidates:\n",
    "            doc_img_path = random.choice(doc_candidates)\n",
    "            selfie_img_path = random.choice(selfie_candidates)\n",
    "            break\n",
    "    label = 1\n",
    "else:\n",
    "    # Negative: doc from one person, selfie from another\n",
    "    doc_img_path, selfie_img_path = None, None\n",
    "    while True:\n",
    "        person1, person2 = random.sample(person_dirs, 2)\n",
    "        files1 = os.listdir(person1)\n",
    "        files2 = os.listdir(person2)\n",
    "        doc_candidates = [os.path.join(person1, f) for f in files1 if any(x in f.lower() for x in [\"id\", \"passport\", \"national\"])]\n",
    "        selfie_candidates = [os.path.join(person2, f) for f in files2 if \"selfie\" in f.lower()]\n",
    "        if doc_candidates and selfie_candidates:\n",
    "            doc_img_path = random.choice(doc_candidates)\n",
    "            selfie_img_path = random.choice(selfie_candidates)\n",
    "            break\n",
    "    label = 0\n",
    "\n",
    "assert doc_img_path is not None and selfie_img_path is not None, \"No valid doc/selfie pair found!\"\n",
    "\n",
    "# Preprocess images\n",
    "doc_img = Image.open(doc_img_path).convert(\"RGB\")\n",
    "selfie_img = Image.open(selfie_img_path).convert(\"RGB\")\n",
    "doc_tensor = transform(doc_img).unsqueeze(0).cpu().numpy()\n",
    "selfie_tensor = transform(selfie_img).unsqueeze(0).cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d5fbee1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pair type: Positive (same person)\n",
      "Random doc: data/Diddata/4/ID_2.jpg\n",
      "Random selfie: data/Diddata/4/Selfie_10.jpg\n",
      "ONNX output (probability): 0.97\n"
     ]
    }
   ],
   "source": [
    "# 4. Run ONNX inference\n",
    "\n",
    "ort_session = ort.InferenceSession(\"Model/best_kyc_siamese.onnx\")\n",
    "outputs = ort_session.run(\n",
    "    None,\n",
    "    {\"doc_img\": doc_tensor, \"selfie_img\": selfie_tensor}\n",
    ")\n",
    "onnx_prob = float(outputs[0].squeeze())  \n",
    "print(f\"Pair type: {'Positive (same person)' if label==1 else 'Negative (different people)'}\")\n",
    "print(f\"Random doc: {doc_img_path}\")\n",
    "print(f\"Random selfie: {selfie_img_path}\")\n",
    "print(\"ONNX output (probability): {:.2f}\".format(onnx_prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4986f762",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc image: inference/ID-2.jpeg\n",
      "Selfie image: inference/Selfie.jpg\n",
      "ONNX output (probability): 0.876\n"
     ]
    }
   ],
   "source": [
    "# Use actual inference images from the inference directory for ONNX model verification\n",
    "\n",
    "# 1. Define the same transform as in training\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((112,112)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
    "])\n",
    "\n",
    "# 2. Set the inference directory and image paths\n",
    "inference_dir = \"inference\"\n",
    "doc_img_path = os.path.join(inference_dir, \"ID-2.jpeg\")\n",
    "selfie_img_path = os.path.join(inference_dir, \"Selfie.jpg\")\n",
    "\n",
    "assert os.path.exists(doc_img_path), f\"Doc image not found: {doc_img_path}\"\n",
    "assert os.path.exists(selfie_img_path), f\"Selfie image not found: {selfie_img_path}\"\n",
    "\n",
    "# 3. Load and preprocess images\n",
    "doc_img = Image.open(doc_img_path).convert(\"RGB\")\n",
    "selfie_img = Image.open(selfie_img_path).convert(\"RGB\")\n",
    "doc_tensor = transform(doc_img).unsqueeze(0).cpu().numpy()\n",
    "selfie_tensor = transform(selfie_img).unsqueeze(0).cpu().numpy()\n",
    "\n",
    "# 4. Run ONNX inference\n",
    "ort_session = ort.InferenceSession(\"Model/best_kyc_siamese.onnx\")\n",
    "outputs = ort_session.run(\n",
    "    None,\n",
    "    {\"doc_img\": doc_tensor, \"selfie_img\": selfie_tensor}\n",
    ")\n",
    "onnx_prob = float(outputs[0].squeeze())\n",
    "print(f\"Doc image: {doc_img_path}\")\n",
    "print(f\"Selfie image: {selfie_img_path}\")\n",
    "print(\"ONNX output (probability): {:.3f}\".format(onnx_prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5ef2ebca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative Pair 1:\n",
      "  Doc image: inference/ID-2.jpeg\n",
      "  Selfie image: data/Diddata/30/Selfie_11.jpg\n",
      "  ONNX output (probability): 0.238\n",
      "\n",
      "Negative Pair 2:\n",
      "  Doc image: inference/ID-2.jpeg\n",
      "  Selfie image: data/Diddata/21/Selfie_10.jpg\n",
      "  ONNX output (probability): 0.975\n",
      "\n",
      "Negative Pair 3:\n",
      "  Doc image: inference/ID-2.jpeg\n",
      "  Selfie image: data/Diddata/23/Selfie_6.jpg\n",
      "  ONNX output (probability): 0.912\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make negative pairs using the inference doc image and random selfie images from the dataset\n",
    "\n",
    "import os\n",
    "import random\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "\n",
    "# 1. Define the same transform as in training\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((112,112)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
    "])\n",
    "\n",
    "# 2. Set the inference doc image path\n",
    "inference_dir = \"inference\"\n",
    "doc_img_path = os.path.join(inference_dir, \"ID-2.jpeg\")\n",
    "assert os.path.exists(doc_img_path), f\"Doc image not found: {doc_img_path}\"\n",
    "\n",
    "# 3. Find random selfie images from other people in the dataset\n",
    "base_dir = \"data/Diddata\"\n",
    "person_dirs = [os.path.join(base_dir, d) for d in os.listdir(base_dir) if os.path.isdir(os.path.join(base_dir, d))]\n",
    "negative_selfies = []\n",
    "\n",
    "for person_dir in person_dirs:\n",
    "    selfie_candidates = [os.path.join(person_dir, f) for f in os.listdir(person_dir) if \"selfie\" in f.lower()]\n",
    "    if selfie_candidates:\n",
    "        negative_selfies.extend(selfie_candidates)\n",
    "\n",
    "# Remove the inference selfie if present\n",
    "inference_selfie_path = os.path.join(inference_dir, \"Selfie.jpg\")\n",
    "negative_selfies = [s for s in negative_selfies if os.path.abspath(s) != os.path.abspath(inference_selfie_path)]\n",
    "\n",
    "# 4. Pick a few random negative selfie images and run ONNX inference\n",
    "num_negatives = 3  # Number of negative pairs to test\n",
    "random.shuffle(negative_selfies)\n",
    "selected_selfies = negative_selfies[:num_negatives]\n",
    "\n",
    "doc_img = Image.open(doc_img_path).convert(\"RGB\")\n",
    "doc_tensor = transform(doc_img).unsqueeze(0).cpu().numpy()\n",
    "\n",
    "ort_session = ort.InferenceSession(\"Model/best_kyc_siamese.onnx\")\n",
    "\n",
    "for idx, selfie_path in enumerate(selected_selfies, 1):\n",
    "    selfie_img = Image.open(selfie_path).convert(\"RGB\")\n",
    "    selfie_tensor = transform(selfie_img).unsqueeze(0).cpu().numpy()\n",
    "    outputs = ort_session.run(\n",
    "        None,\n",
    "        {\"doc_img\": doc_tensor, \"selfie_img\": selfie_tensor}\n",
    "    )\n",
    "    onnx_prob = float(outputs[0].squeeze())\n",
    "    print(f\"Negative Pair {idx}:\")\n",
    "    print(f\"  Doc image: {doc_img_path}\")\n",
    "    print(f\"  Selfie image: {selfie_path}\")\n",
    "    print(f\"  ONNX output (probability): {onnx_prob:.3f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2e76ca57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model IR version: 7\n",
      "Producer: pytorch v2.7.0\n",
      "Graph name: main_graph\n",
      "Inputs:\n",
      " - doc_img, shape: [0, 3, 112, 112]\n",
      " - selfie_img, shape: [0, 3, 112, 112]\n",
      "\n",
      "Outputs:\n",
      " - output, shape: [0]\n",
      "\n",
      "Total nodes: 36\n",
      "Unique node types: {'Gemm', 'Relu', 'Concat', 'MaxPool', 'Sigmoid', 'Flatten', 'Squeeze', 'Conv', 'GlobalAveragePool'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = onnx.load(\"Model/best_kyc_siamese.onnx\")\n",
    "\n",
    "print(f\"Model IR version: {model.ir_version}\")\n",
    "print(f\"Producer: {model.producer_name} v{model.producer_version}\")\n",
    "print(f\"Graph name: {model.graph.name}\")\n",
    "print(f\"Inputs:\")\n",
    "for i in model.graph.input:\n",
    "    print(f\" - {i.name}, shape: {[d.dim_value for d in i.type.tensor_type.shape.dim]}\")\n",
    "\n",
    "print(f\"\\nOutputs:\")\n",
    "for o in model.graph.output:\n",
    "    print(f\" - {o.name}, shape: {[d.dim_value for d in o.type.tensor_type.shape.dim]}\")\n",
    "\n",
    "print(f\"\\nTotal nodes: {len(model.graph.node)}\")\n",
    "print(f\"Unique node types: {set(n.op_type for n in model.graph.node)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "didenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
