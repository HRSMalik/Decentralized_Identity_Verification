{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6056a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pwd\n",
    "import os\n",
    "os.chdir(\"../\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ce51f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830f65c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "zip_path = \"data/Diddata.zip\"\n",
    "extract_to = \"data\"\n",
    "\n",
    "os.makedirs(extract_to, exist_ok=True)\n",
    "\n",
    "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "    members = zip_ref.namelist()\n",
    "    for member in tqdm(members, desc=\"Extracting\"):\n",
    "        zip_ref.extract(member, extract_to)\n",
    "\n",
    "print(f\"Extracted to {extract_to}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6e8849",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "base_dir = \"data/Diddata\"\n",
    "dirs = [d for d in os.listdir(base_dir) if os.path.isdir(os.path.join(base_dir, d))]\n",
    "dirs.sort()  # Sort for consistent numbering\n",
    "\n",
    "for idx, dirname in enumerate(dirs, start=1):\n",
    "    src = os.path.join(base_dir, dirname)\n",
    "    dst = os.path.join(base_dir, str(idx))\n",
    "    os.rename(src, dst)\n",
    "\n",
    "print(f\"Renamed {len(dirs)} directories to 1-{len(dirs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c595672e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, torch, shutil, numpy as np\n",
    "from glob import glob; from PIL import Image\n",
    "from torch.utils.data import random_split, Dataset, DataLoader\n",
    "from torchvision import transforms as T\n",
    "torch.manual_seed(42)\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, root, transformations = None, im_files = [\".jpg\", \".jpeg\", \".png\"]):\n",
    "        \n",
    "        self.transformations = transformations\n",
    "        self.im_paths = sorted(glob(f\"{root}/*/*{[im_file for im_file in im_files]}\"))\n",
    "        \n",
    "        self.cls_names, self.cls_counts, count, data_count = {}, {}, 0, 0\n",
    "        for idx, im_path in enumerate(self.im_paths):\n",
    "            class_name = self.get_class(im_path)\n",
    "            if class_name not in self.cls_names: self.cls_names[class_name] = count; self.cls_counts[class_name] = 1; count += 1\n",
    "            else: self.cls_counts[class_name] += 1\n",
    "        \n",
    "    def get_class(self, path): return os.path.dirname(path).split(\"/\")[-1]\n",
    "    \n",
    "    def __len__(self): return len(self.im_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        im_path = self.im_paths[idx]\n",
    "        im = Image.open(im_path).convert(\"RGB\")\n",
    "        gt = self.cls_names[self.get_class(im_path)]\n",
    "        \n",
    "        if self.transformations is not None: im = self.transformations(im)\n",
    "        \n",
    "        return im, gt\n",
    "    \n",
    "def get_dls(root, transformations, bs, split = [0.9, 0.05, 0.05], ns = 4):\n",
    "    \n",
    "    ds = CustomDataset(root = root, transformations = transformations)\n",
    "    \n",
    "    total_len = len(ds)\n",
    "    tr_len = int(total_len * split[0])\n",
    "    vl_len = int(total_len * split[1])\n",
    "    ts_len = total_len - (tr_len + vl_len)\n",
    "    \n",
    "    tr_ds, vl_ds, ts_ds = random_split(dataset = ds, lengths = [tr_len, vl_len, ts_len])\n",
    "    \n",
    "    tr_dl, val_dl, ts_dl = DataLoader(tr_ds, batch_size = bs, shuffle = True, num_workers = ns), DataLoader(vl_ds, batch_size = bs, shuffle = False, num_workers = ns), DataLoader(ts_ds, batch_size = 1, shuffle = False, num_workers = ns)\n",
    "    \n",
    "    return tr_dl, val_dl, ts_dl, ds.cls_names\n",
    "\n",
    "root = \"data/Diddata\"\n",
    "mean, std, im_size = [0.485, 0.456, 0.406], [0.229, 0.224, 0.225], 224\n",
    "tfs = T.Compose([T.Resize((im_size, im_size)), T.ToTensor(), T.Normalize(mean = mean, std = std)])\n",
    "tr_dl, val_dl, ts_dl, classes = get_dls(root = root, transformations = tfs, bs = 32)\n",
    "\n",
    "print(len(tr_dl)); print(len(val_dl)); print(len(ts_dl)); print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ce420d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def tensor_2_im(t, t_type = \"rgb\"):\n",
    "    \n",
    "    gray_tfs = T.Compose([T.Normalize(mean = [ 0.], std = [1/0.5]), T.Normalize(mean = [-0.5], std = [1])])\n",
    "    rgb_tfs = T.Compose([T.Normalize(mean = [ 0., 0., 0. ], std = [ 1/0.229, 1/0.224, 1/0.225 ]), T.Normalize(mean = [ -0.485, -0.456, -0.406 ], std = [ 1., 1., 1. ])])\n",
    "    \n",
    "    invTrans = gray_tfs if t_type == \"gray\" else rgb_tfs \n",
    "    \n",
    "    return (invTrans(t) * 255).detach().squeeze().cpu().permute(1,2,0).numpy().astype(np.uint8) if t_type == \"gray\" else (invTrans(t) * 255).detach().cpu().permute(1,2,0).numpy().astype(np.uint8)\n",
    "\n",
    "def visualize(data, n_ims, rows, cmap = None, cls_names = None):\n",
    "    \n",
    "    assert cmap in [\"rgb\", \"gray\"]\n",
    "    if cmap == \"rgb\": cmap = \"viridis\"\n",
    "    \n",
    "    plt.figure(figsize = (20, 10))\n",
    "    indekslar = [random.randint(0, len(data) - 1) for _ in range(n_ims)]\n",
    "    for idx, indeks in enumerate(indekslar):\n",
    "        \n",
    "        im, gt = data[indeks]\n",
    "        # Start plot\n",
    "        plt.subplot(rows, n_ims // rows, idx + 1)\n",
    "        if cmap: plt.imshow(tensor_2_im(im, cmap), cmap=cmap)\n",
    "        else: plt.imshow(tensor_2_im(im))\n",
    "        plt.axis('off')\n",
    "        if cls_names is not None: plt.title(f\"GT -> {cls_names[int(gt)]}\")\n",
    "        else: plt.title(f\"GT -> {gt}\")\n",
    "            \n",
    "visualize(tr_dl.dataset, 20, 4, \"rgb\", list(classes.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69fec2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize(val_dl.dataset, 20, 4, \"rgb\", list(classes.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0ec260",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize(ts_dl.dataset, 20, 4, \"rgb\", list(classes.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d9a8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_analysis(root, transformations):\n",
    "    \n",
    "    ds = CustomDataset(root = root, transformations = transformations)\n",
    "    cls_counts, width, text_width = ds.cls_counts,  0.7, 0.05\n",
    "    text_height = 2\n",
    "    cls_names = list(cls_counts.keys()); counts = list(cls_counts.values())\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize = (20, 10))\n",
    "    indices = np.arange(len(counts))\n",
    "\n",
    "    ax.bar(indices, counts, width, color = \"firebrick\")\n",
    "    ax.set_xlabel(\"Class Names\", color = \"red\")\n",
    "    ax.set_xticklabels(cls_names, rotation = 60)\n",
    "    ax.set(xticks = indices, xticklabels = cls_names)\n",
    "    ax.set_ylabel(\"Data Counts\", color = \"red\")\n",
    "    ax.set_title(f\"Dataset Class Imbalance Analysis\")\n",
    "\n",
    "    for i, v in enumerate(counts): ax.text(i - text_width, v + text_height, str(v), color = \"royalblue\")\n",
    "    \n",
    "data_analysis(root = root, transformations = tfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e040d20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import torch\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "# from torchvision import transforms, models\n",
    "# from PIL import Image\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "\n",
    "# # 1. Dataset for pairs\n",
    "# class IDSelfiePairDataset(Dataset):\n",
    "#     def __init__(self, root, transform=None):\n",
    "#         self.pairs = []\n",
    "#         self.labels = []\n",
    "#         self.transform = transform\n",
    "#         # Assumes each dir: [id.jpg, selfie.jpg]\n",
    "#         for doc_type in os.listdir(root):\n",
    "#             doc_dir = os.path.join(root, doc_type)\n",
    "#             if not os.path.isdir(doc_dir): continue\n",
    "#             files = os.listdir(doc_dir)\n",
    "#             id_img = [f for f in files if 'id' in f.lower()]\n",
    "#             selfie_img = [f for f in files if 'selfie' in f.lower()]\n",
    "#             if id_img and selfie_img:\n",
    "#                 self.pairs.append((os.path.join(doc_dir, id_img[0]), os.path.join(doc_dir, selfie_img[0])))\n",
    "#                 self.labels.append(1)  # positive pair\n",
    "#             # Optionally add negative pairs (different people)\n",
    "#             # ... (for a real dataset, add negative pairs here)\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.pairs)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         img1 = Image.open(self.pairs[idx][0]).convert(\"RGB\")\n",
    "#         img2 = Image.open(self.pairs[idx][1]).convert(\"RGB\")\n",
    "#         if self.transform:\n",
    "#             img1 = self.transform(img1)\n",
    "#             img2 = self.transform(img2)\n",
    "#         label = torch.tensor(self.labels[idx], dtype=torch.float32)\n",
    "#         return img1, img2, label\n",
    "\n",
    "# # 2. Simple Siamese Network\n",
    "# class SiameseNetwork(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "#         self.cnn = models.resnet18(pretrained=True)\n",
    "#         self.cnn.fc = nn.Identity()\n",
    "#         self.fc = nn.Sequential(\n",
    "#             nn.Linear(512*2, 256),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(256, 1)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x1, x2):\n",
    "#         f1 = self.cnn(x1)\n",
    "#         f2 = self.cnn(x2)\n",
    "#         out = torch.cat([f1, f2], dim=1)\n",
    "#         out = self.fc(out)\n",
    "#         return torch.sigmoid(out).squeeze(1)\n",
    "\n",
    "# # 3. Training setup\n",
    "# transform = transforms.Compose([\n",
    "#     transforms.Resize((224,224)),\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
    "# ])\n",
    "# dataset = IDSelfiePairDataset(root=\"data/Diddata\", transform=transform)\n",
    "# dataloader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model = SiameseNetwork().to(device)\n",
    "# criterion = nn.BCELoss()\n",
    "# optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# # 4. Training loop\n",
    "# epochs = 5\n",
    "# for epoch in range(epochs):\n",
    "#     model.train()\n",
    "#     running_loss = 0.0\n",
    "#     for img1, img2, label in dataloader:\n",
    "#         img1, img2, label = img1.to(device), img2.to(device), label.to(device)\n",
    "#         optimizer.zero_grad()\n",
    "#         output = model(img1, img2)\n",
    "#         loss = criterion(output, label)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         running_loss += loss.item()\n",
    "#     print(f\"Epoch {epoch+1}, Loss: {running_loss/len(dataloader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202a14c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4edcf80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Custom Dataset for KYC Verification (with positive and negative pairs)\n",
    "class KYCVerificationDataset(Dataset):\n",
    "    def __init__(self, root, transform=None, neg_ratio=1.0):\n",
    "        self.transform = transform\n",
    "        self.pairs = []\n",
    "        self.labels = []\n",
    "        self.person_dirs = [os.path.join(root, d) for d in os.listdir(root) if os.path.isdir(os.path.join(root, d))]\n",
    "        self.person_dirs.sort()\n",
    "        # Collect all doc and selfie images for each person\n",
    "        self.docs = []\n",
    "        self.selfies = []\n",
    "        for person_dir in self.person_dirs:\n",
    "            doc_imgs = [os.path.join(person_dir, f) for f in os.listdir(person_dir) if 'id' in f.lower() or 'passport' in f.lower() or 'national' in f.lower()]\n",
    "            selfie_imgs = [os.path.join(person_dir, f) for f in os.listdir(person_dir) if 'selfie' in f.lower()]\n",
    "            self.docs.append(doc_imgs)\n",
    "            self.selfies.append(selfie_imgs)\n",
    "        # Positive pairs (same person)\n",
    "        for i, (doc_list, selfie_list) in enumerate(zip(self.docs, self.selfies)):\n",
    "            for doc in doc_list:\n",
    "                for selfie in selfie_list:\n",
    "                    self.pairs.append((doc, selfie))\n",
    "                    self.labels.append(1)\n",
    "        # Negative pairs (different people)\n",
    "        num_neg = int(len(self.pairs) * neg_ratio)\n",
    "        for _ in range(num_neg):\n",
    "            i, j = random.sample(range(len(self.person_dirs)), 2)\n",
    "            if self.docs[i] and self.selfies[j]:\n",
    "                doc = random.choice(self.docs[i])\n",
    "                selfie = random.choice(self.selfies[j])\n",
    "                self.pairs.append((doc, selfie))\n",
    "                self.labels.append(0)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        doc_path, selfie_path = self.pairs[idx]\n",
    "        doc_img = Image.open(doc_path).convert(\"RGB\")\n",
    "        selfie_img = Image.open(selfie_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            doc_img = self.transform(doc_img)\n",
    "            selfie_img = self.transform(selfie_img)\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.float32)\n",
    "        return doc_img, selfie_img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9153f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Siamese Network Definition\n",
    "class SiameseNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.cnn = models.resnet18(pretrained=True)\n",
    "        self.cnn.fc = nn.Identity()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(512*2, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        f1 = self.cnn(x1)\n",
    "        f2 = self.cnn(x2)\n",
    "        out = torch.cat([f1, f2], dim=1)\n",
    "        out = self.fc(out)\n",
    "        return torch.sigmoid(out).squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f0d228",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# Cell 4: DataLoader and Training Setup\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
    "])\n",
    "dataset = KYCVerificationDataset(root=\"data/Diddata\", transform=transform, neg_ratio=1.0)\n",
    "\n",
    "all_indices = list(range(len(dataset)))\n",
    "# First split: train+val vs test\n",
    "trainval_indices, test_indices = train_test_split(all_indices, test_size=0.1, random_state=42, shuffle=True)\n",
    "# Second split: train vs val\n",
    "train_indices, val_indices = train_test_split(trainval_indices, test_size=0.1, random_state=42, shuffle=True)\n",
    "\n",
    "from torch.utils.data import Subset, DataLoader\n",
    "\n",
    "train_dataset = Subset(dataset, train_indices)\n",
    "val_dataset = Subset(dataset, val_indices)\n",
    "test_dataset = Subset(dataset, test_indices)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6e7a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = SiameseNetwork().to(device)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f4746d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Training Loop with torchmetrics, tqdm, and Model Checkpointing\n",
    "\n",
    "from torchmetrics.classification import BinaryAccuracy, BinaryPrecision, BinaryRecall, BinaryF1Score\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Initialize lists before the training loop\n",
    "train_losses, val_losses = [], []\n",
    "train_accs, val_accs = [], []\n",
    "train_f1s, val_f1s = [], []\n",
    "\n",
    "epochs = 5\n",
    "best_val_f1 = 0.0\n",
    "for epoch in range(epochs):  # <-- Only one loop here!\n",
    "    # --- Training ---\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    acc_metric = BinaryAccuracy().to(device)\n",
    "    prec_metric = BinaryPrecision().to(device)\n",
    "    rec_metric = BinaryRecall().to(device)\n",
    "    f1_metric = BinaryF1Score().to(device)\n",
    "\n",
    "    for doc_img, selfie_img, label in tqdm(train_loader, desc=f\"Train Epoch {epoch+1}\"):\n",
    "        doc_img, selfie_img, label = doc_img.to(device), selfie_img.to(device), label.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(doc_img, selfie_img)\n",
    "        loss = criterion(output, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        preds = (output > 0.5).float()\n",
    "        acc_metric.update(preds, label)\n",
    "        prec_metric.update(preds, label)\n",
    "        rec_metric.update(preds, label)\n",
    "        f1_metric.update(preds, label)\n",
    "\n",
    "    train_acc = acc_metric.compute().item()\n",
    "    train_prec = prec_metric.compute().item()\n",
    "    train_rec = rec_metric.compute().item()\n",
    "    train_f1 = f1_metric.compute().item()\n",
    "    avg_train_loss = running_loss / len(train_loader)\n",
    "    print(f\"Train Epoch {epoch+1}: Loss={avg_train_loss:.4f}, Acc={train_acc:.4f}, Prec={train_prec:.4f}, Rec={train_rec:.4f}, F1={train_f1:.4f}\")\n",
    "\n",
    "    # --- Validation ---\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_acc_metric = BinaryAccuracy().to(device)\n",
    "    val_prec_metric = BinaryPrecision().to(device)\n",
    "    val_rec_metric = BinaryRecall().to(device)\n",
    "    val_f1_metric = BinaryF1Score().to(device)\n",
    "    with torch.no_grad():\n",
    "        for doc_img, selfie_img, label in val_loader:\n",
    "            doc_img, selfie_img, label = doc_img.to(device), selfie_img.to(device), label.to(device)\n",
    "            output = model(doc_img, selfie_img)\n",
    "            loss = criterion(output, label)\n",
    "            val_loss += loss.item()\n",
    "            preds = (output > 0.5).float()\n",
    "            val_acc_metric.update(preds, label)\n",
    "            val_prec_metric.update(preds, label)\n",
    "            val_rec_metric.update(preds, label)\n",
    "            val_f1_metric.update(preds, label)\n",
    "\n",
    "    val_acc = val_acc_metric.compute().item()\n",
    "    val_prec = val_prec_metric.compute().item()\n",
    "    val_rec = val_rec_metric.compute().item()\n",
    "    val_f1 = val_f1_metric.compute().item()\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    print(f\"Val Epoch {epoch+1}: Loss={avg_val_loss:.4f}, Acc={val_acc:.4f}, Prec={val_prec:.4f}, Rec={val_rec:.4f}, F1={val_f1:.4f}\")\n",
    "\n",
    "    # Save best model by validation F1\n",
    "    if val_f1 > best_val_f1:\n",
    "        best_val_f1 = val_f1\n",
    "        torch.save(model.state_dict(), \"best_kyc_siamese.pt\")\n",
    "        print(f\"Best model saved at epoch {epoch+1} with Val F1={val_f1:.4f}\")\n",
    "\n",
    "    # ---- APPEND METRICS HERE ----\n",
    "    train_losses.append(avg_train_loss)\n",
    "    val_losses.append(avg_val_loss)\n",
    "    train_accs.append(train_acc)\n",
    "    val_accs.append(val_acc)\n",
    "    train_f1s.append(train_f1)\n",
    "    val_f1s.append(val_f1)\n",
    "\n",
    "# After training loop, plot the metrics\n",
    "epochs_range = range(1, epochs+1)\n",
    "\n",
    "plt.figure(figsize=(16,5))\n",
    "plt.subplot(1,3,1)\n",
    "plt.plot(epochs_range, train_losses, label='Train Loss')\n",
    "plt.plot(epochs_range, val_losses, label='Val Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Loss')\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "plt.plot(epochs_range, train_accs, label='Train Acc')\n",
    "plt.plot(epochs_range, val_accs, label='Val Acc')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.title('Accuracy')\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "plt.plot(epochs_range, train_f1s, label='Train F1')\n",
    "plt.plot(epochs_range, val_f1s, label='Val F1')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.legend()\n",
    "plt.title('F1 Score')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254809f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
    "\n",
    "# --- Test Set Evaluation with Confusion Matrix and Classification Report ---\n",
    "\n",
    "# Load the best model\n",
    "model.load_state_dict(torch.load(\"best_kyc_siamese.pt\"))\n",
    "model.eval()\n",
    "\n",
    "all_labels = []\n",
    "all_preds = []\n",
    "test_loss = 0.0\n",
    "\n",
    "test_acc_metric = BinaryAccuracy().to(device)\n",
    "test_prec_metric = BinaryPrecision().to(device)\n",
    "test_rec_metric = BinaryRecall().to(device)\n",
    "test_f1_metric = BinaryF1Score().to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for doc_img, selfie_img, label in tqdm(test_loader, desc=\"Testing\"):\n",
    "        doc_img, selfie_img, label = doc_img.to(device), selfie_img.to(device), label.to(device)\n",
    "        output = model(doc_img, selfie_img)\n",
    "        loss = criterion(output, label)\n",
    "        test_loss += loss.item()\n",
    "        preds = (output > 0.5).float()\n",
    "        all_labels.extend(label.cpu().numpy())\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        test_acc_metric.update(preds, label)\n",
    "        test_prec_metric.update(preds, label)\n",
    "        test_rec_metric.update(preds, label)\n",
    "        test_f1_metric.update(preds, label)\n",
    "\n",
    "avg_test_loss = test_loss / len(test_loader)\n",
    "test_acc = test_acc_metric.compute().item()\n",
    "test_prec = test_prec_metric.compute().item()\n",
    "test_rec = test_rec_metric.compute().item()\n",
    "test_f1 = test_f1_metric.compute().item()\n",
    "\n",
    "print(f\"Best Model Test Results: Loss={avg_test_loss:.4f}, Acc={test_acc:.4f}, Prec={test_prec:.4f}, Rec={test_rec:.4f}, F1={test_f1:.4f}\")\n",
    "\n",
    "# --- Confusion Matrix ---\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Negative\", \"Positive\"])\n",
    "os.makedirs(\"plot\", exist_ok=True)\n",
    "plt.figure(figsize=(5,5))\n",
    "disp.plot(cmap=plt.cm.Blues, values_format='d')\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.savefig(\"plot/confusion_matrix.png\")\n",
    "plt.show()\n",
    "\n",
    "# --- Classification Report ---\n",
    "report = classification_report(all_labels, all_preds, target_names=[\"Negative\", \"Positive\"])\n",
    "print(report)\n",
    "with open(\"plot/classification_report.txt\", \"w\") as f:\n",
    "    f.write(report)\n",
    "\n",
    "# --- Save best stats to a file ---\n",
    "with open(\"plot/best_model_stats.txt\", \"w\") as f:\n",
    "    f.write(f\"Best Model Test Results:\\n\")\n",
    "    f.write(f\"Loss: {avg_test_loss:.4f}\\n\")\n",
    "    f.write(f\"Accuracy: {test_acc:.4f}\\n\")\n",
    "    f.write(f\"Precision: {test_prec:.4f}\\n\")\n",
    "    f.write(f\"Recall: {test_rec:.4f}\\n\")\n",
    "    f.write(f\"F1 Score: {test_f1:.4f}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "didenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
